{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArseniyKoz/uni.neuralnetworks/blob/main/ltsm_Dostoevskiy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDEH20jS0L9c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "SAMR_fK61DJt",
        "outputId": "fa672121-2cdc-4226-8e8b-6bfca20cd3e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Федор Михайлович Достоевский\\nБедные люди\\nОх уж эти мне сказочники! Нет чтобы написать что-нибудь полезное, приятное, усладительное, а то всю подноготную в земле вырывают!.. Вот уж запретил бы им писать! Ну, на что это похоже: читаешь… невольно задумываешься, — а там всякая дребедень и пойдет в голову; право бы, запретил им писать; так-таки просто вовсе бы запретил.\\nКн. В. Ф. Одоевский\\nАпреля 8-го\\nБесценная моя Варвара Алексеевна!\\nВчера я был счастлив, чрезмерно счастлив, донельзя счастлив! Вы хоть раз в жизни, упрямица, меня послушались. Вечером, часов в восемь, просыпаюсь (вы знаете, маточка, что я часочек-другой люблю поспать после должности), свечку достал, приготовляю бумаги, чиню перо, вдруг, невзначай, подымаю глаза, — право, у меня сердце вот так и запрыгало! Так вы-таки поняли, чего мне хотелось, чего сердчишку моему хотелось! Вижу, уголочек занавески у окна вашего загнут и прицеплен к горшку с бальзамином, точнехонько так, как я вам тогда намекал; тут же показалось мне, что и '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "with open('dostoevsky.txt', 'r') as f:\n",
        "  text = f.read()\n",
        "text[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOsZyp8V1D8r"
      },
      "outputs": [],
      "source": [
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mzLQa6c2cCz",
        "outputId": "0dacafa8-ddf8-4a9d-8f38-6f59e32fbe35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([121,  17,  36,  67,  26,  59,  24,  15,   0,   1,  64,  84,  67,\n",
              "        20,  15, 100,  59,  55,  67,   9,  86,  67,  17,  20,   9,  27,\n",
              "        15,  64,  89,  88,  17,  36,  92,  49,  17,  59,  84, 129,  36,\n",
              "        15,  89,  29,   0,  59,  95,  93,  59,   7,  86,  15,  59, 115,\n",
              "        92,  17,  59,   9,  27,   1, 114,  67, 100,  92,  15,  27,  15,\n",
              "        74,  59, 108,  17,  86,  59, 100,  86,  67, 126,  49,  59,  92,\n",
              "         1,  83,  15,   9,   1,  86, 131,  59, 100,  86,  67, 122,  92,\n",
              "        15, 126,  95,  36, 131,  59,  83,  67,  84])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "encoded = np.array([char2int[ch] for ch in text])\n",
        "encoded[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL1JFzf22saq"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "  one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "  one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "  one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "  return one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mMfSnUT224_",
        "outputId": "8d21b0e6-1aa1-4e69-c975-a5a26a6eaf43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ]
        }
      ],
      "source": [
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "print(one_hot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiho8ToD3Cga"
      },
      "outputs": [],
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "  '''Create a generator that returns batches of size\n",
        "  batch_size x seq_length from arr.\n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  arr: Array you want to make batches from\n",
        "  batch_size: Batch size, the number of sequences per batch\n",
        "  seq_length: Number of encoded chars in a sequence\n",
        "  '''\n",
        "\n",
        "  ## TODO: Get the number of batches we can make\n",
        "  n_batches = len(arr) // (batch_size*seq_length)\n",
        "\n",
        "  ## TODO: Keep only enough characters to make full batches\n",
        "  arr = arr[:batch_size*n_batches*seq_length]\n",
        "\n",
        "  ## TODO: Reshape into batch_size rows\n",
        "  arr = arr.reshape(batch_size, -1)\n",
        "\n",
        "  ## Iterate over the batches using a window of size seq_length\n",
        "  for n in range(0, arr.shape[1], seq_length):\n",
        "    # The features\n",
        "    x = arr[:, n:n+seq_length]\n",
        "    # The targets, shifted by one\n",
        "    y = np.zeros_like(x)\n",
        "    try:\n",
        "      y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "    except IndexError:\n",
        "      y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "    yield x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vOl7E1V3S0l",
        "outputId": "745c99a2-0ce8-404d-c582-e7a7304b2fcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x\n",
            " [[121  17  36  67  26  59  24  15   0   1  64  84  67  20  15 100  59  55\n",
            "   67   9  86  67  17  20   9  27  15  64  89  88  17  36  92  49  17  59\n",
            "   84 129  36  15  89  29   0  59  95  93  59   7  86  15]\n",
            " [ 84  10  59  95  84  49 126   1  43   9 131  59 100  17  26  17 114  59\n",
            "    9  15  84  95  10  59 100  86  67 126  49  59  95  36  17  26  93   1\n",
            "   86 131  59  20  59  36  95  34  17  59  92   1  27  15]\n",
            " [ 59  20  59  83  26  15  20  49 100  92  67 115  59  95  98  84  17  59\n",
            "   92   1  36  67  84  98  67  11  59 110   1  27 122  86  67  59  84  95\n",
            "  100  34  17  59  86   1 115  10  59  98  36  17  59  83]\n",
            " [  9 131  59  67   9  86   1  20   1  86 131   9  43  59  86  17  83  17\n",
            "   26 131  11  59 109   9  84  15  59 126  49  59  92  17  59  98  26  95\n",
            "    9  86  92  67  59 126  49  84  67  10  59  43  59 126]\n",
            " [ 36 131  59  92   1 115  17  26  17  92  15  17 115  59  67  92  15  59\n",
            "   92  17  59 115  67  98  84  15  59   7  86  67  98  67  59   9  36  17\n",
            "   84   1  86 131  11  59  19  86  67  59  86   1  27  10]\n",
            " [ 59   9  17  98  67  36  92  43  59  36  67  84  98  67  59  26   1 114\n",
            "   36  95 115  49  20   1  84   1  11  59  89  57  59 108  95  10  59  15\n",
            "   59 100  17 115  59  93  17  59  27  67  92 100  15  84]\n",
            " [ 92  17  59   9  67  20   9  17 115  59  17   8  17  59  17  98  67  59\n",
            "   83  67  92  15 115   1 129  10  59  92  17  59   9  67  20   9  17 115\n",
            "   59  17  98  67  59 114  92   1 129  11  59  30  92   1]\n",
            " [ 15  59  83  67  83   1  84  67  10  59   1  59  20 115  17   9  86  17\n",
            "   59  15  59  20  59   9  20  67  15   0  59  95 100  15  86  17  84  17\n",
            "   64  10  59  83  26  17  15 115  95   8  17   9  86  20]]\n",
            "\n",
            "y\n",
            " [[ 17  36  67  26  59  24  15   0   1  64]\n",
            " [ 10  59  95  84  49 126   1  43   9 131]\n",
            " [ 20  59  83  26  15  20  49 100  92  67]\n",
            " [131  59  67   9  86   1  20   1  86 131]\n",
            " [131  59  92   1 115  17  26  17  92  15]\n",
            " [  9  17  98  67  36  92  43  59  36  67]\n",
            " [ 17  59   9  67  20   9  17 115  59  17]\n",
            " [ 59  83  67  83   1  84  67  10  59   1]]\n"
          ]
        }
      ],
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)\n",
        "\n",
        "print('x\\n', x)\n",
        "print('\\ny\\n', y[:10, :10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU5GJKeK7tmM",
        "outputId": "40df4b25-93b9-44c8-8c0e-1f34deaca0e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU!\n"
          ]
        }
      ],
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "  print('Training on GPU!')\n",
        "else:\n",
        "  print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0O_I24b761_"
      },
      "outputs": [],
      "source": [
        "class CharRNN(nn.Module):\n",
        "  def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.3  , lr=0.001):\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_layers = n_layers\n",
        "    self.n_hidden = n_hidden\n",
        "    self.lr = lr\n",
        "\n",
        "    # creating character dictionaries\n",
        "    self.chars = tokens\n",
        "    self.int2char = dict(enumerate(self.chars))\n",
        "    self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "    self.vocab_size = len(char2int)\n",
        "\n",
        "    ## TODO: define the layers of the model\n",
        "    self.dropout = nn.Dropout(self.drop_prob)\n",
        "\n",
        "    self.lstm = torch.nn.LSTM(self.vocab_size, self.n_hidden, num_layers=self.n_layers, dropout=self.drop_prob, batch_first=True)\n",
        "    #self.lstm = torch.nn.LSTM(self.vocab_size, hidden_size = self.n_hidden, num_layers=self.n_layers, dropout=self.drop_prob, bidirectional=True, batch_first=True)\n",
        "    self.fc = nn.Linear(self.n_hidden, self.vocab_size) #<linear>\n",
        "    #self.fc = nn.Linear(2*self.n_hidden, self.vocab_size)\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    ''' Forward pass through the network.\n",
        "    These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "\n",
        "    ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "    #print(hidden)\n",
        "    r_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "\n",
        "    ## TODO: pass through a dropout layer\n",
        "    out = self.dropout(r_output)\n",
        "\n",
        "    # Stack up LSTM outputs using view\n",
        "    # you may need to use contiguous to reshape the output\n",
        "    out = out.contiguous().view(-1,self.n_hidden)\n",
        "\n",
        "    ## TODO: put x through the fully-connected layer\n",
        "    out = self.fc(out)\n",
        "\n",
        "\n",
        "    # return the final output and the hidden state\n",
        "    return out, hidden\n",
        "\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    ''' Initializes hidden state '''\n",
        "    # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "    # initialized to zero, for hidden state and cell state of LSTM\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "    if (train_on_gpu):\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "\n",
        "    return hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wzTaKx9QNP8"
      },
      "outputs": [],
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network\n",
        "        Arguments\n",
        "        ---------\n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    '''\n",
        "    net.train()\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr, amsgrad=True)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "      opt,\n",
        "      patience=5,\n",
        "      verbose=True,\n",
        "      factor=0.5\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "    if(train_on_gpu):\n",
        "      net.cuda()\n",
        "\n",
        "    loss_avg = []\n",
        "\n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "      #net.train()\n",
        "    # initialize hidden state\n",
        "      h = net.init_hidden(batch_size)\n",
        "\n",
        "      for x, y in get_batches(data, batch_size, seq_length):\n",
        "        #print(x, y)\n",
        "        counter += 1\n",
        "\n",
        "        # One-hot encode our data and make them Torch tensors\n",
        "        x = one_hot_encode(x, n_chars)\n",
        "        inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "        if(train_on_gpu):\n",
        "          inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "        loss.backward()\n",
        "\n",
        "        loss_avg.append(loss.item())\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        opt.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "          # Get validation loss\n",
        "          val_h = net.init_hidden(batch_size)\n",
        "          val_losses = []\n",
        "\n",
        "          mean_loss = np.mean(loss_avg)\n",
        "          scheduler.step(mean_loss)\n",
        "          loss_avg = []\n",
        "\n",
        "          net.eval()\n",
        "          for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "            inputs, targets = x, y\n",
        "            if(train_on_gpu):\n",
        "              inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            output, val_h = net(inputs, val_h)\n",
        "            val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "          net.train() # reset to train mode after iterationg through validation data\n",
        "\n",
        "          print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                \"Step: {}...\".format(counter),\n",
        "                \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUHFscnORImY",
        "outputId": "70bebfa3-64b1-4653-cd01-1df84d042126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (lstm): LSTM(133, 128, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (fc): Linear(in_features=128, out_features=133, bias=True)\n",
            ")\n",
            "Epoch: 3/32... Step: 1000... Loss: 1.8191... Val Loss: 2.0977\n",
            "Epoch: 6/32... Step: 2000... Loss: 1.8368... Val Loss: 1.9204\n",
            "Epoch: 8/32... Step: 3000... Loss: 1.6389... Val Loss: 1.8572\n",
            "Epoch: 11/32... Step: 4000... Loss: 1.6196... Val Loss: 1.8422\n",
            "Epoch: 13/32... Step: 5000... Loss: 1.5582... Val Loss: 1.8061\n",
            "Epoch: 16/32... Step: 6000... Loss: 1.5135... Val Loss: 1.8074\n",
            "Epoch: 19/32... Step: 7000... Loss: 1.6806... Val Loss: 1.7944\n",
            "Epoch: 21/32... Step: 8000... Loss: 1.7507... Val Loss: 1.7785\n",
            "Epoch: 24/32... Step: 9000... Loss: 1.5056... Val Loss: 1.7887\n",
            "Epoch: 26/32... Step: 10000... Loss: 1.6463... Val Loss: 1.7759\n",
            "Epoch: 29/32... Step: 11000... Loss: 1.5612... Val Loss: 1.7819\n",
            "Epoch: 32/32... Step: 12000... Loss: 1.5941... Val Loss: 1.7739\n"
          ]
        }
      ],
      "source": [
        "## TODO: set your model hyperparameters\n",
        "# define and print the net\n",
        "n_hidden = 128\n",
        "n_layers = 2\n",
        "dropout = 0.2\n",
        "net = CharRNN(chars, n_hidden, n_layers, dropout)\n",
        "print(net)\n",
        "batch_size = 8\n",
        "seq_length = 128\n",
        "n_epochs = 32 # start small if you are just testing initial behavior\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.01, print_every=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mtfj4gNeN7l"
      },
      "outputs": [],
      "source": [
        "model_name = 'rnn_x_epoch.net'\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "with open(model_name, 'wb') as f:\n",
        "  torch.save(checkpoint, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsNzh9SIeUAz"
      },
      "outputs": [],
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "  ''' Given a character, predict the next character.\n",
        "  Returns the predicted character and the hidden state.\n",
        "  '''\n",
        "\n",
        "  # tensor inputs\n",
        "  x = np.array([[net.char2int[char]]])\n",
        "  x = one_hot_encode(x, len(net.chars))\n",
        "  inputs = torch.from_numpy(x)\n",
        "\n",
        "  if(train_on_gpu):\n",
        "    inputs = inputs.cuda()\n",
        "\n",
        "  # detach hidden state from history\n",
        "  h = tuple([each.data for each in h])\n",
        "  # get the output of the model\n",
        "  out, h = net(inputs, h)\n",
        "  # get the character probabilities\n",
        "  p = F.softmax(out, dim=1).data\n",
        "  if(train_on_gpu):\n",
        "    p = p.cpu() # move to cpu\n",
        "\n",
        "  # get top characters\n",
        "  if top_k is None:\n",
        "    top_ch = np.arange(len(net.chars))\n",
        "  else:\n",
        "    p, top_ch = p.topk(top_k)\n",
        "    top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "  # select the likely next character with some element of randomness\n",
        "  p = p.numpy().squeeze()\n",
        "  char = np.random.choice(top_ch, p=p/p.sum())\n",
        "\n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return net.int2char[char], h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKl6j5DxechZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6be211c5-fa40-4d6f-cddf-be70bbb27ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Анна нет их и не будет и нужно с самое себя положить; но он на какая-то из сердце меня оставления, не скажу с первую подразумитель об деньги с сада, что я стало быть, будет, что ваша, крестью, и столько стал сердечко с вас стоял оставают, как бы вы меня нет от материя мысль. В такого... Но вы с вами, что он не мог не больчик вы по перед беспререссивание своим моем молоды, кресть в долго просто, и собразоваться несчастие, и все пилать. Ведь вот чувством собора, что они об карту стало бы все это наделная и по потила, как-то благодетерный матушку своих добра, но в послишать предлагать веселость. Но он не просил из собрание. Я сам мне было нужно вас бы не более на садинко по своему.\n",
            "Ваша\n",
            "Вечера нет, Настенька, потому что он божий десять странная строхольше, но вот одна стало божий мой, чтобы он, которая была были последние моим и сегорости на нашего денег с ним во сказать, и только не была на всю меня ни знаю; всю слова, краснет, на своей, который посла остальна на нас прядная володенется, что \n"
          ]
        }
      ],
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "  if(train_on_gpu):\n",
        "    net.cuda()\n",
        "  else:\n",
        "    net.cpu()\n",
        "\n",
        "  net.eval() # eval mode\n",
        "\n",
        "  # First off, run through the prime characters\n",
        "  chars = [ch for ch in prime]\n",
        "  h = net.init_hidden(1)\n",
        "  for ch in prime:\n",
        "    char, h = predict(net, ch, h, top_k=top_k)\n",
        "  chars.append(char)\n",
        "\n",
        "  # Now pass in the previous character and get a new one\n",
        "  for ii in range(size):\n",
        "    char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "    chars.append(char)\n",
        "  return ''.join(chars)\n",
        "print(sample(net, 1000, prime='Анна', top_k=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nC-iGjFeuKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6326e4fa-1687-406e-c92d-16af370fb28e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-ac01ad47a0f9>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(f)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "И он сказал на него доме, не было. Прошелт с душеть, потому что я полный мы вызатернуться, чиновник, и пословиче. Он достойную. Я-то ведь никогда не можете, что он вот вам одиночита письмо неприятно и только всё сладекое посмеле, как ни вымутет, не могу; ну, на нем один печать и ваши постушом. Посмотрил, что вас, что я под сердце! Обо всех после друга моим думало, что я вам совершенно больше на месте полнул из подобности платка мою и потолки, по просту не молоду, что мне сердиться. Я большая, когда вы не стало быть, в сегодня, кончивый последную мечтать и с нимом, на подрожало себь! Она отказывая моим вас, и нужно стало ужасную в самом деле он про наши на восмоверенно со мною странного держал и не стало быть, он боялся, что-нибудь, чтоб все на нам слезы наделанность ни наприличению про малицым верите, чем был привидений нось не могла не сказали, что бы было надобно и блина, подумом, послушайте, чтобы она одна, к совершенно, как будто был простино, потому что вот, не быть, и пример, и в самый другом\n"
          ]
        }
      ],
      "source": [
        "with open('rnn_x_epoch.net', 'rb') as f:\n",
        "  checkpoint = torch.load(f)\n",
        "\n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])\n",
        "# Sample using a loaded model\n",
        "print(sample(loaded, 1000, prime=\"И он сказал \",  top_k=5))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}